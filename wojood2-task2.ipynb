{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8303505,"sourceType":"datasetVersion","datasetId":4932833},{"sourceId":8329960,"sourceType":"datasetVersion","datasetId":4946162},{"sourceId":8338995,"sourceType":"datasetVersion","datasetId":4952622}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install seqeval transformers datasets tokenizers seqeval evaluate\n! pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T20:11:29.297268Z","iopub.execute_input":"2024-05-06T20:11:29.297626Z","iopub.status.idle":"2024-05-06T20:11:45.411900Z","shell.execute_reply.started":"2024-05-06T20:11:29.297593Z","shell.execute_reply":"2024-05-06T20:11:45.410804Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=7679254343dd6bca0a018461108649a4c4f38d0c9dd1122f3703a2ccb79a81dd\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport datasets\nimport json\nimport numpy as np \nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForTokenClassification, BertConfig, DataCollatorForTokenClassification, BertTokenizerFast, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:11:45.413902Z","iopub.execute_input":"2024-05-06T20:11:45.414293Z","iopub.status.idle":"2024-05-06T20:12:04.219938Z","shell.execute_reply.started":"2024-05-06T20:11:45.414255Z","shell.execute_reply":"2024-05-06T20:12:04.219089Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-06 20:11:55.414583: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 20:11:55.414683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 20:11:55.608303: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_arabic(text):\n    alif_maksura_to_yeh = re.sub(r'[ÙŠÙ‰]', 'ÙŠ', text)\n    teh_marbuta_to_heh = re.sub(r'Ø©', 'Ù‡', alif_maksura_to_yeh)\n    alifs_normalized = re.sub(r'[Ø£Ø¥Ø¢]', 'Ø§', teh_marbuta_to_heh)\n    kafs_normalized = re.sub(r'Ú©', 'Ùƒ', alifs_normalized)\n    text_cleaned = re.sub(r'[\\u064B-\\u065F]', '', kafs_normalized)\n\n    return text_cleaned","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:42:17.461303Z","iopub.execute_input":"2024-05-06T19:42:17.461709Z","iopub.status.idle":"2024-05-06T19:42:17.467636Z","shell.execute_reply.started":"2024-05-06T19:42:17.461679Z","shell.execute_reply":"2024-05-06T19:42:17.466686Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# def process_json_file_nested(file_path, tag_to_int):\n#     with open(file_path, 'r') as file:\n#         data = json.load(file)\n\n#     token_data_layers = []\n#     label_data_layers = []\n#     max_depth = 0\n\n#     for sentence in data:\n#         for token_info in sentence['tokens']:\n#             max_depth = max(max_depth, len(token_info['tags']))\n\n#     for _ in range(max_depth):\n#         token_data_layers.append([])\n#         label_data_layers.append([])\n\n#     for sentence in data:\n#         for depth in range(max_depth):\n#             token_list = []\n#             label_list = []\n#             for token_info in sentence['tokens']:\n#                 token = token_info['token']\n#                 if depth < len(token_info['tags']):\n#                     tag_info = token_info['tags'][depth]\n#                     value = tag_info['value']\n#                 else:\n#                     value = \"O\"\n#                 token_list.append(token)\n#                 label_list.append(tag_to_int[value])\n#             token_data_layers[depth].append(token_list)\n#             label_data_layers[depth].append(label_list)\n\n#     datasets = []\n#     for i in range(max_depth):\n#         datasets.append({'tokens': token_data_layers[i], 'labels': label_data_layers[i]})\n\n#     return datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:20:36.280109Z","iopub.execute_input":"2024-05-06T13:20:36.280928Z","iopub.status.idle":"2024-05-06T13:20:36.286093Z","shell.execute_reply.started":"2024-05-06T13:20:36.280895Z","shell.execute_reply":"2024-05-06T13:20:36.285060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_json_file_flat(file_path, tag_to_int):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    token_data = []\n    label_data = []\n    \n    for sentence in data:\n        tokens = []\n        combined_labels = []\n        for token_info in sentence['tokens']:\n            token = token_info['token']\n            tokens.append(token)\n            \n            # Create a combined tag from all layers available for this token\n            tag_combination = '-'.join([tag['value'] for tag in token_info['tags']])\n            if tag_combination not in tag_to_int:\n                tag_to_int[tag_combination] = len(tag_to_int)  # Assign new unique integer if not in dict\n            combined_labels.append(tag_to_int[tag_combination])\n        \n        token_data.append(tokens)\n        label_data.append(combined_labels)\n    \n    return {'tokens': token_data, 'labels': label_data}, tag_to_int\n\n# Use this function to prepare your data\n# combined_dataset, updated_tag_to_int = process_json_file_flat('path_to_your_file.json', {})\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:04.221779Z","iopub.execute_input":"2024-05-06T20:12:04.222836Z","iopub.status.idle":"2024-05-06T20:12:04.230504Z","shell.execute_reply.started":"2024-05-06T20:12:04.222800Z","shell.execute_reply":"2024-05-06T20:12:04.229626Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/ner-nested/split70.json'\nvalid_path = '/kaggle/input/ner-nested/split10.json'\ntest_path = '/kaggle/input/nested-ner-test/split20-nested-unlabeled.json'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:04.231508Z","iopub.execute_input":"2024-05-06T20:12:04.231758Z","iopub.status.idle":"2024-05-06T20:12:04.256443Z","shell.execute_reply.started":"2024-05-06T20:12:04.231735Z","shell.execute_reply":"2024-05-06T20:12:04.255562Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef extract_unique_tags(file_paths):\n    unique_tags = set()\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n            for sentence in data:\n                for token_info in sentence['tokens']:\n                    for tag_info in token_info['tags']:\n                        unique_tags.add(tag_info['value'])\n                        if tag_info.get('tags'):\n                            for nested_tag in tag_info['tags']:\n                                unique_tags.add(nested_tag['value'])\n    return unique_tags\n\ndef calculate_tag_frequencies(labels, label_ids):\n    tag_counts = {}\n    for sublist in labels:\n        for label in sublist:\n            tag_name = label_ids[label]\n            if tag_name in tag_counts:\n                tag_counts[tag_name] += 1\n            else:\n                tag_counts[tag_name] = 1\n    return tag_counts\n\n# Define file paths for your training, validation, and test datasets\nfile_paths = [train_path, valid_path, test_path]\nunique_tags = extract_unique_tags(file_paths)\ntag_to_int = {tag: idx for idx, tag in enumerate(unique_tags)}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:04.258568Z","iopub.execute_input":"2024-05-06T20:12:04.259344Z","iopub.status.idle":"2024-05-06T20:12:10.224374Z","shell.execute_reply.started":"2024-05-06T20:12:04.259310Z","shell.execute_reply":"2024-05-06T20:12:10.223385Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets for the first layer only\ntrain_datasets, tag_to_int = process_json_file_flat(train_path, tag_to_int)\nvalid_datasets, _ = process_json_file_flat(valid_path, tag_to_int)\ntest_datasets, _ = process_json_file_flat(test_path, tag_to_int)\nlabel_ids = {idx: label for label, idx in tag_to_int.items()}\ntag_counts = calculate_tag_frequencies(train_datasets['labels'], label_ids)\n\n\n\ntrain_ds = Dataset.from_dict(train_datasets)\nvalid_ds = Dataset.from_dict(valid_datasets)\ntest_ds = Dataset.from_dict(test_datasets)\n\n# Select the first layer (index 0)\n# train_dataset = train_datasets[0]\n# valid_dataset = valid_datasets[0]\n# test_dataset = test_datasets[0]\n# tag_to_int = {label: idx for idx, label in enumerate(set([lbl for sublist in train_dataset['labels'] for lbl in sublist]))}\n# label_ids = {idx: label for label, idx in tag_to_int.items()}\n\n# train_ds = Dataset.from_dict(train_dataset)\n# valid_ds = Dataset.from_dict(valid_dataset)\n# test_ds = Dataset.from_dict(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:10.225923Z","iopub.execute_input":"2024-05-06T20:12:10.226376Z","iopub.status.idle":"2024-05-06T20:12:15.996318Z","shell.execute_reply.started":"2024-05-06T20:12:10.226323Z","shell.execute_reply":"2024-05-06T20:12:15.995546Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"all_labels = [label for sublist in train_datasets['labels'] for label in sublist]\n\n# Count the frequency of each unique label\nfrom collections import Counter\nlabel_frequencies = Counter(all_labels)\n\n# Print the frequencies\nfor label_id, freq in label_frequencies.items():\n    label_name = next(key for key, value in tag_to_int.items() if value == label_id)\n    print(f'Label: {label_name}, Frequency: {freq}')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:11:00.877352Z","iopub.execute_input":"2024-05-06T20:11:00.877731Z","iopub.status.idle":"2024-05-06T20:11:00.928849Z","shell.execute_reply.started":"2024-05-06T20:11:00.877701Z","shell.execute_reply":"2024-05-06T20:11:00.927903Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Label: O, Frequency: 254050\nLabel: B-CARDINAL, Frequency: 1291\nLabel: B-ORG, Frequency: 10572\nLabel: I-ORG, Frequency: 10070\nLabel: B-DATE, Frequency: 10705\nLabel: B-LANGUAGE, Frequency: 139\nLabel: B-NORP, Frequency: 3585\nLabel: B-PERS, Frequency: 4515\nLabel: I-PERS, Frequency: 4698\nLabel: B-OCC, Frequency: 3716\nLabel: I-DATE, Frequency: 39338\nLabel: B-GPE, Frequency: 8052\nLabel: B-EVENT, Frequency: 1845\nLabel: I-EVENT, Frequency: 1560\nLabel: I-OCC-B-PERS, Frequency: 6\nLabel: I-OCC-I-PERS, Frequency: 2\nLabel: I-CARDINAL, Frequency: 350\nLabel: B-FAC, Frequency: 560\nLabel: I-FAC, Frequency: 472\nLabel: B-LOC, Frequency: 747\nLabel: B-MONEY, Frequency: 148\nLabel: I-MONEY-B-CURR, Frequency: 137\nLabel: B-ORDINAL, Frequency: 2739\nLabel: I-LANGUAGE, Frequency: 4\nLabel: B-TIME, Frequency: 309\nLabel: I-TIME, Frequency: 250\nLabel: I-ORG-B-GPE, Frequency: 4482\nLabel: B-MONEY-B-CURR, Frequency: 24\nLabel: I-GPE, Frequency: 4751\nLabel: I-LOC, Frequency: 463\nLabel: I-NORP, Frequency: 1668\nLabel: B-ORG-B-GPE, Frequency: 11\nLabel: I-OCC, Frequency: 2126\nLabel: I-OCC-B-ORG, Frequency: 1688\nLabel: I-OCC-I-ORG, Frequency: 1548\nLabel: B-QUANTITY-B-UNIT, Frequency: 4\nLabel: I-FAC-B-GPE, Frequency: 144\nLabel: I-ORG-I-GPE, Frequency: 400\nLabel: I-FAC-B-LOC, Frequency: 21\nLabel: B-PERCENT, Frequency: 92\nLabel: I-PERCENT, Frequency: 141\nLabel: I-NORP-B-GPE, Frequency: 261\nLabel: I-EVENT-B-GPE, Frequency: 507\nLabel: B-UNIT, Frequency: 3\nLabel: B-QUANTITY, Frequency: 39\nLabel: I-QUANTITY-B-UNIT, Frequency: 35\nLabel: I-QUANTITY-I-UNIT, Frequency: 4\nLabel: B-NORP-B-GPE, Frequency: 1\nLabel: I-LOC-B-GPE, Frequency: 235\nLabel: I-LOC-I-GPE, Frequency: 111\nLabel: B-CURR, Frequency: 15\nLabel: I-OCC-B-GPE, Frequency: 819\nLabel: B-WEBSITE, Frequency: 411\nLabel: I-NORP-I-GPE, Frequency: 210\nLabel: I-MONEY, Frequency: 138\nLabel: I-NORP-B-ORG, Frequency: 260\nLabel: I-OCC-B-NORP, Frequency: 73\nLabel: I-QUANTITY, Frequency: 14\nLabel: I-OCC-I-ORG-B-ORG, Frequency: 15\nLabel: I-OCC-I-ORG-I-ORG, Frequency: 18\nLabel: I-ORG-B-PERS, Frequency: 87\nLabel: I-ORG-I-PERS, Frequency: 49\nLabel: I-ORDINAL, Frequency: 346\nLabel: I-OCC-I-ORG-B-GPE, Frequency: 694\nLabel: I-FAC-I-GPE, Frequency: 93\nLabel: I-FAC-B-PERS, Frequency: 67\nLabel: I-NORP-B-PERS, Frequency: 86\nLabel: B-ORG-B-PERS, Frequency: 6\nLabel: I-ORG-B-ORG, Frequency: 404\nLabel: B-OCC-I-EVENT, Frequency: 1\nLabel: I-OCC-I-EVENT, Frequency: 5\nLabel: I-OCC-I-NORP-B-GPE, Frequency: 6\nLabel: I-FAC-I-PERS, Frequency: 35\nLabel: I-ORG-B-NORP, Frequency: 157\nLabel: I-ORG-B-EVENT, Frequency: 59\nLabel: I-ORG-I-EVENT, Frequency: 28\nLabel: I-ORG-I-EVENT-B-GPE, Frequency: 14\nLabel: I-OCC-I-ORG-I-GPE, Frequency: 138\nLabel: I-OCC-I-GPE, Frequency: 392\nLabel: I-EVENT-B-DATE, Frequency: 616\nLabel: I-NORP-I-PERS, Frequency: 75\nLabel: I-GPE-B-GPE, Frequency: 378\nLabel: I-GPE-I-GPE, Frequency: 361\nLabel: I-EVENT-B-ORG, Frequency: 95\nLabel: I-EVENT-I-ORG, Frequency: 126\nLabel: I-ORG-I-ORG-B-GPE, Frequency: 335\nLabel: I-FAC-B-ORG, Frequency: 24\nLabel: I-EVENT-B-ORDINAL, Frequency: 430\nLabel: I-EVENT-I-DATE, Frequency: 217\nLabel: I-OCC-I-NORP-B-ORG, Frequency: 2\nLabel: I-OCC-I-NORP-I-ORG, Frequency: 2\nLabel: I-OCC-I-NORP-I-ORG-B-GPE, Frequency: 2\nLabel: I-GPE-B-NORP, Frequency: 51\nLabel: I-NORP-B-EVENT, Frequency: 11\nLabel: I-NORP-I-EVENT, Frequency: 17\nLabel: I-NORP-I-EVENT-B-GPE, Frequency: 4\nLabel: I-NORP-I-EVENT-I-GPE, Frequency: 1\nLabel: I-FAC-B-FAC, Frequency: 22\nLabel: I-FAC-I-FAC, Frequency: 23\nLabel: I-GPE-I-NORP, Frequency: 43\nLabel: I-GPE-B-PERS, Frequency: 156\nLabel: I-ORG-I-GPE-B-NORP, Frequency: 1\nLabel: I-ORG-I-GPE-I-NORP, Frequency: 1\nLabel: I-NORP-I-GPE-B-NORP, Frequency: 1\nLabel: I-NORP-I-GPE-I-NORP, Frequency: 1\nLabel: I-GPE-I-PERS, Frequency: 12\nLabel: I-EVENT-I-GPE, Frequency: 101\nLabel: B-PERS-B-OCC, Frequency: 3\nLabel: I-EVENT-B-PERS, Frequency: 48\nLabel: I-EVENT-I-ORG-B-ORG, Frequency: 11\nLabel: I-EVENT-I-ORG-I-ORG-B-GPE, Frequency: 11\nLabel: I-EVENT-B-FAC, Frequency: 144\nLabel: I-NORP-I-ORG, Frequency: 175\nLabel: I-EVENT-I-PERS, Frequency: 49\nLabel: I-EVENT-I-FAC, Frequency: 16\nLabel: I-FAC-I-ORG, Frequency: 29\nLabel: I-FAC-I-ORG-B-GPE, Frequency: 9\nLabel: I-FAC-I-ORG-I-GPE, Frequency: 7\nLabel: I-EVENT-I-ORG-B-GPE, Frequency: 16\nLabel: I-PERS-B-ORDINAL, Frequency: 9\nLabel: I-NORP-B-LOC, Frequency: 9\nLabel: I-NORP-I-LOC, Frequency: 2\nLabel: I-NORP-I-ORG-B-GPE, Frequency: 36\nLabel: I-NORP-I-ORG-I-GPE, Frequency: 17\nLabel: I-OCC-I-GPE-B-GPE, Frequency: 2\nLabel: I-OCC-I-GPE-I-GPE, Frequency: 2\nLabel: I-OCC-B-LOC, Frequency: 27\nLabel: I-OCC-I-LOC, Frequency: 17\nLabel: I-OCC-B-OCC, Frequency: 61\nLabel: I-OCC-I-OCC, Frequency: 33\nLabel: I-EVENT-B-LOC, Frequency: 16\nLabel: I-ORG-I-EVENT-B-ORDINAL, Frequency: 6\nLabel: I-NORP-B-FAC, Frequency: 3\nLabel: I-FAC-B-NORP, Frequency: 11\nLabel: I-ORG-I-ORG, Frequency: 69\nLabel: I-ORG-I-NORP-B-GPE, Frequency: 2\nLabel: I-ORG-I-NORP, Frequency: 44\nLabel: I-OCC-I-OCC-B-NORP, Frequency: 3\nLabel: I-NORP-I-GPE-B-PERS, Frequency: 34\nLabel: B-PERS-B-GPE, Frequency: 1\nLabel: I-OCC-I-LOC-B-GPE, Frequency: 4\nLabel: I-ORG-B-ORDINAL, Frequency: 2\nLabel: I-ORG-B-LOC, Frequency: 17\nLabel: I-ORG-I-LOC-B-GPE, Frequency: 1\nLabel: I-OCC-I-ORG-B-CARDINAL, Frequency: 1\nLabel: I-OCC-I-ORG-B-ORDINAL, Frequency: 1\nLabel: I-GPE-B-ORG, Frequency: 1\nLabel: I-GPE-I-ORG-B-GPE, Frequency: 1\nLabel: B-EVENT-B-DATE, Frequency: 5\nLabel: I-NORP-I-LOC-B-GPE, Frequency: 1\nLabel: I-OCC-I-ORG-I-ORG-B-GPE, Frequency: 4\nLabel: I-OCC-I-OCC-B-ORG, Frequency: 24\nLabel: I-OCC-I-OCC-I-ORG-B-GPE, Frequency: 9\nLabel: I-FAC-I-NORP, Frequency: 2\nLabel: I-FAC-I-NORP-B-PERS, Frequency: 4\nLabel: I-EVENT-I-FAC-B-PERS, Frequency: 2\nLabel: I-EVENT-I-LOC, Frequency: 9\nLabel: I-EVENT-B-OCC, Frequency: 17\nLabel: I-EVENT-I-OCC-B-ORG, Frequency: 4\nLabel: I-EVENT-I-OCC-I-ORG, Frequency: 2\nLabel: I-EVENT-I-ORG-I-GPE, Frequency: 1\nLabel: I-EVENT-B-NORP, Frequency: 29\nLabel: I-OCC-I-OCC-B-GPE, Frequency: 22\nLabel: I-OCC-B-EVENT, Frequency: 2\nLabel: I-ORG-I-NORP-B-FAC, Frequency: 38\nLabel: I-NORP-I-ORG-B-ORG, Frequency: 7\nLabel: I-NORP-I-ORG-I-ORG-B-GPE, Frequency: 1\nLabel: I-EVENT-I-NORP-B-GPE, Frequency: 3\nLabel: I-EVENT-I-NORP-I-GPE, Frequency: 3\nLabel: I-EVENT-I-FAC-B-LOC, Frequency: 1\nLabel: I-OCC-I-ORG-B-LOC, Frequency: 55\nLabel: I-OCC-I-ORG-I-LOC, Frequency: 54\nLabel: I-FAC-I-LOC-B-GPE, Frequency: 3\nLabel: I-PERS-I-OCC-B-GPE, Frequency: 1\nLabel: I-FAC-I-GPE-B-GPE, Frequency: 1\nLabel: I-FAC-I-GPE-I-GPE, Frequency: 1\nLabel: I-LOC-B-LOC, Frequency: 69\nLabel: I-MONEY-I-CURR, Frequency: 35\nLabel: I-LOC-I-GPE-B-GPE, Frequency: 1\nLabel: I-LOC-I-GPE-I-GPE, Frequency: 1\nLabel: I-EVENT-I-OCC, Frequency: 4\nLabel: I-ORG-B-OCC, Frequency: 8\nLabel: I-ORG-I-OCC-B-GPE, Frequency: 4\nLabel: I-ORG-I-OCC-I-GPE, Frequency: 1\nLabel: I-EVENT-I-NORP, Frequency: 20\nLabel: I-LOC-I-LOC, Frequency: 5\nLabel: I-OCC-B-ORDINAL, Frequency: 4\nLabel: I-OCC-I-ORG-B-NORP, Frequency: 25\nLabel: I-OCC-I-ORG-I-NORP, Frequency: 28\nLabel: I-OCC-I-OCC-I-GPE, Frequency: 13\nLabel: I-OCC-B-FAC, Frequency: 3\nLabel: I-OCC-I-FAC-B-PERS, Frequency: 1\nLabel: I-ORG-B-FAC, Frequency: 5\nLabel: I-ORG-I-FAC-B-LOC, Frequency: 1\nLabel: I-ORG-I-FAC-I-LOC, Frequency: 1\nLabel: I-PERS-B-NORP, Frequency: 3\nLabel: I-PERS-I-NORP, Frequency: 3\nLabel: I-OCC-I-NORP-I-GPE, Frequency: 1\nLabel: I-ORG-I-LOC, Frequency: 13\nLabel: I-OCC-I-FAC, Frequency: 2\nLabel: I-OCC-I-OCC-I-ORG-B-LOC, Frequency: 1\nLabel: I-OCC-I-OCC-I-ORG-I-LOC, Frequency: 1\nLabel: I-FAC-I-LOC, Frequency: 1\nLabel: I-EVENT-I-GPE-B-PERS, Frequency: 3\nLabel: I-ORG-I-GPE-B-GPE, Frequency: 8\nLabel: I-ORG-I-GPE-I-GPE, Frequency: 4\nLabel: I-GPE-B-DATE, Frequency: 1\nLabel: I-ORG-I-FAC, Frequency: 1\nLabel: I-GPE-B-LOC, Frequency: 2\nLabel: I-GPE-I-LOC, Frequency: 1\nLabel: I-NORP-B-ORDINAL, Frequency: 2\nLabel: I-EVENT-I-LOC-B-PERS, Frequency: 1\nLabel: I-EVENT-I-LOC-I-PERS, Frequency: 1\nLabel: I-ORG-I-GPE-B-PERS, Frequency: 7\nLabel: B-UNIT-B-QUANTITY, Frequency: 3\nLabel: B-LAW, Frequency: 368\nLabel: I-LAW, Frequency: 726\nLabel: I-LAW-B-NORP, Frequency: 3\nLabel: I-LOC-B-PERS, Frequency: 1\nLabel: I-LOC-B-FAC, Frequency: 1\nLabel: I-LOC-I-FAC, Frequency: 1\nLabel: I-EVENT-B-CARDINAL, Frequency: 14\nLabel: I-FAC-B-ORDINAL, Frequency: 1\nLabel: I-EVENT-I-NORP-B-ORG, Frequency: 1\nLabel: I-OCC-I-ORG-I-NORP-B-ORG, Frequency: 2\nLabel: I-OCC-I-ORG-I-NORP-I-ORG, Frequency: 6\nLabel: I-NORP-I-ORG-B-NORP, Frequency: 2\nLabel: I-NORP-I-ORG-I-NORP, Frequency: 2\nLabel: I-OCC-I-OCC-I-ORG-B-NORP, Frequency: 1\nLabel: I-OCC-I-OCC-I-ORG-I-NORP, Frequency: 1\nLabel: I-OCC-I-OCC-I-ORG, Frequency: 22\nLabel: I-OCC-I-OCC-I-ORG-I-GPE, Frequency: 1\nLabel: I-NORP-I-ORG-I-ORG-B-NORP, Frequency: 1\nLabel: I-NORP-I-ORG-I-ORG-I-NORP, Frequency: 1\nLabel: I-NORP-B-CARDINAL, Frequency: 2\nLabel: I-NORP-I-CARDINAL, Frequency: 1\nLabel: B-ORG-B-ORG, Frequency: 1\nLabel: I-OCC-I-LOC-B-LOC, Frequency: 1\nLabel: I-WEBSITE, Frequency: 1328\nLabel: I-WEBSITE-B-ORG, Frequency: 1\nLabel: I-NORP-I-ORG-I-ORG, Frequency: 3\nLabel: I-EVENT-I-ORDINAL, Frequency: 1\nLabel: I-OCC-I-OCC-I-ORG-B-ORG, Frequency: 1\nLabel: I-OCC-I-OCC-I-ORG-I-ORG-B-GPE, Frequency: 1\nLabel: I-NORP-B-DATE, Frequency: 1\nLabel: I-CURR, Frequency: 3\nLabel: I-EVENT-I-LOC-B-LOC, Frequency: 1\nLabel: I-LAW-B-ORG, Frequency: 3\nLabel: I-OCC-I-OCC-B-LOC, Frequency: 8\nLabel: I-OCC-I-OCC-I-LOC, Frequency: 8\nLabel: B-PRODUCT, Frequency: 61\nLabel: I-PRODUCT, Frequency: 86\nLabel: I-ORG-I-OCC, Frequency: 4\nLabel: I-LAW-B-DATE, Frequency: 9\nLabel: I-LAW-I-DATE, Frequency: 7\nLabel: I-LAW-B-CARDINAL, Frequency: 1\nLabel: I-EVENT-I-LOC-B-GPE, Frequency: 8\nLabel: I-LAW-B-ORDINAL, Frequency: 234\nLabel: I-EVENT-I-FAC-B-GPE, Frequency: 1\nLabel: I-LAW-I-ORG-B-GPE, Frequency: 1\nLabel: I-LAW-I-ORG-I-GPE, Frequency: 1\nLabel: I-LAW-B-GPE, Frequency: 1\nLabel: I-LOC-B-NORP, Frequency: 2\nLabel: I-LOC-I-LOC-B-GPE, Frequency: 2\nLabel: I-EVENT-I-OCC-B-PERS, Frequency: 1\nLabel: I-ORG-I-LOC-B-LOC, Frequency: 5\nLabel: I-LAW-B-EVENT, Frequency: 4\nLabel: I-LAW-I-EVENT-B-ORG, Frequency: 2\nLabel: I-NORP-I-LOC-B-LOC, Frequency: 2\nLabel: I-LAW-I-EVENT-I-ORG, Frequency: 1\nLabel: I-EVENT-I-PERS-B-ORDINAL, Frequency: 1\nLabel: I-DATE-B-PERS, Frequency: 3\nLabel: I-ORG-B-CARDINAL, Frequency: 2\nLabel: I-LOC-B-ORG, Frequency: 3\nLabel: I-PRODUCT-B-ORDINAL, Frequency: 10\nLabel: I-EVENT-B-PRODUCT, Frequency: 1\nLabel: I-EVENT-I-PRODUCT, Frequency: 2\nLabel: I-DATE-B-EVENT, Frequency: 3\nLabel: I-DATE-I-EVENT, Frequency: 3\nLabel: I-DATE-I-PERS, Frequency: 1\nLabel: I-NORP-I-ORG-B-PERS, Frequency: 3\nLabel: I-OCC-B-TIME, Frequency: 1\nLabel: I-EVENT-I-OCC-I-ORG-B-GPE, Frequency: 1\nLabel: I-WEBSITE-B-CARDINAL, Frequency: 1\nLabel: I-WEBSITE-B-DATE, Frequency: 4\nLabel: I-WEBSITE-I-DATE, Frequency: 8\nLabel: I-WEBSITE-B-PERS, Frequency: 1\nLabel: I-WEBSITE-B-NORP, Frequency: 1\nLabel: I-WEBSITE-B-ORG-B-PERS, Frequency: 1\nLabel: I-WEBSITE-I-ORG, Frequency: 4\nLabel: I-WEBSITE-B-ORDINAL, Frequency: 3\nLabel: I-WEBSITE-B-LOC, Frequency: 1\nLabel: I-WEBSITE-I-LOC, Frequency: 2\nLabel: I-WEBSITE-B-GPE, Frequency: 7\nLabel: I-WEBSITE-I-GPE, Frequency: 2\nLabel: B-WEBSITE-B-ORG, Frequency: 1\nLabel: I-WEBSITE-B-ORG-B-GPE, Frequency: 1\nLabel: I-DATE-B-ORG, Frequency: 1\nLabel: I-DATE-I-ORG-B-PERS, Frequency: 1\nLabel: I-NORP-B-OCC, Frequency: 16\nLabel: I-LAW-I-ORG, Frequency: 2\nLabel: I-LAW-I-EVENT, Frequency: 2\nLabel: I-LOC-I-ORG, Frequency: 3\nLabel: I-ORG-B-DATE, Frequency: 3\nLabel: I-OCC-B-LANGUAGE, Frequency: 1\nLabel: I-ORG-I-NORP-B-ORDINAL, Frequency: 2\nLabel: I-EVENT-I-ORG-B-NORP, Frequency: 2\nLabel: I-EVENT-I-CARDINAL, Frequency: 6\nLabel: I-EVENT-B-TIME, Frequency: 1\nLabel: I-EVENT-I-TIME, Frequency: 3\nLabel: I-MONEY-B-UNIT, Frequency: 1\nLabel: I-FAC-I-GPE-B-PERS, Frequency: 4\nLabel: I-OCC-I-EVENT-B-GPE, Frequency: 1\nLabel: I-OCC-I-EVENT-B-DATE, Frequency: 1\nLabel: I-OCC-I-EVENT-I-DATE, Frequency: 2\nLabel: I-OCC-I-ORG-B-FAC, Frequency: 1\nLabel: I-OCC-I-ORG-I-FAC-B-LOC, Frequency: 1\nLabel: I-OCC-I-ORG-I-FAC-I-LOC, Frequency: 1\nLabel: I-NORP-I-FAC, Frequency: 1\nLabel: I-OCC-I-NORP, Frequency: 1\nLabel: I-ORG-I-FAC-B-PERS, Frequency: 2\nLabel: I-OCC-I-OCC-I-LOC-B-GPE, Frequency: 1\nLabel: I-NORP-I-EVENT-B-DATE, Frequency: 1\nLabel: I-NORP-I-GPE-B-GPE, Frequency: 1\nLabel: I-NORP-I-GPE-I-GPE, Frequency: 1\nLabel: I-NORP-I-ORG-I-GPE-B-GPE, Frequency: 1\nLabel: I-NORP-I-ORG-I-GPE-I-GPE, Frequency: 1\nLabel: I-LOC-I-LOC-I-GPE, Frequency: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples, label_all_tokens=True):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"labels\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:15.998435Z","iopub.execute_input":"2024-05-06T20:12:15.998881Z","iopub.status.idle":"2024-05-06T20:12:16.006106Z","shell.execute_reply.started":"2024-05-06T20:12:15.998846Z","shell.execute_reply":"2024-05-06T20:12:16.005199Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model_path = \"aubmindlab/bert-base-arabertv2\"\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\ndata_collator = DataCollatorForTokenClassification(tokenizer) \nmetric = datasets.load_metric(\"seqeval\") ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:16.007523Z","iopub.execute_input":"2024-05-06T20:12:16.007865Z","iopub.status.idle":"2024-05-06T20:12:18.054740Z","shell.execute_reply.started":"2024-05-06T20:12:16.007833Z","shell.execute_reply":"2024-05-06T20:12:18.053993Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a2038da900f4b9f8d694656624e45e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/720k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a3690ae400438f91f09dda41b028a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89bebcfa81d446d8af1b05d4f4a63281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"917286aacbaf482182e8096993e153dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ad40e987cd43939f1c807054e9d8d2"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_34/2027728299.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = datasets.load_metric(\"seqeval\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21ded3a0762417e87977dfc447bae37"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_prepare(dataset):\n    return dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:20:53.442878Z","iopub.execute_input":"2024-05-06T13:20:53.443153Z","iopub.status.idle":"2024-05-06T13:20:53.447517Z","shell.execute_reply.started":"2024-05-06T13:20:53.443130Z","shell.execute_reply":"2024-05-06T13:20:53.446586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds.map(tokenize_and_align_labels, batched=True)\ntokenized_valid_ds = valid_ds.map(tokenize_and_align_labels, batched=True)\ntokenized_test_ds = test_ds.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:12:18.056389Z","iopub.execute_input":"2024-05-06T20:12:18.056638Z","iopub.status.idle":"2024-05-06T20:12:23.221303Z","shell.execute_reply.started":"2024-05-06T20:12:18.056615Z","shell.execute_reply":"2024-05-06T20:12:23.220362Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23125 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f43f0074384aa1875ea1d234250639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3304 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b9fce7634a4bccbee5fc03cc9f2484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6606 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df03a045363749ad89cdf5ce6da8249a"}},"metadata":{}}]},{"cell_type":"code","source":"def calculate_weights(tag_counts):\n    total_tags = sum(tag_counts.values())\n    weights = {tag: total_tags / count for tag, count in tag_counts.items()}\n\n    max_weight = max(weights.values())\n    weights_normalized = {tag: weight / max_weight for tag, weight in weights.items()}\n\n    return list(weights_normalized.values())\n\nclass_weights = torch.tensor(calculate_weights(tag_counts), dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:18.919149Z","iopub.execute_input":"2024-05-06T19:43:18.919945Z","iopub.status.idle":"2024-05-06T19:43:18.926373Z","shell.execute_reply.started":"2024-05-06T19:43:18.919908Z","shell.execute_reply":"2024-05-06T19:43:18.925259Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"weighted_loss = nn.CrossEntropyLoss(weight=class_weights)\n\nclass ArabNERModelWithWeightedLoss(AutoModelForTokenClassification):\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        outputs = super().forward(input_ids, attention_mask=attention_mask, **kwargs)\n        if labels is not None:\n            loss = weighted_loss(outputs.logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs[1:]\n        return outputs\n\nmodel = ArabNERModelWithWeightedLoss.from_pretrained(model_path, num_labels=len(label_ids.values()))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:20.154351Z","iopub.execute_input":"2024-05-06T19:43:20.154702Z","iopub.status.idle":"2024-05-06T19:43:20.683531Z","shell.execute_reply.started":"2024-05-06T19:43:20.154678Z","shell.execute_reply":"2024-05-06T19:43:20.682560Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nclass ArabNERModel(AutoModelForTokenClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        outputs = super().forward(input_ids, attention_mask=attention_mask, **kwargs)\n        if labels is not None:\n            loss = self.loss(outputs.logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs[1:]\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:13:02.315085Z","iopub.execute_input":"2024-05-06T20:13:02.315596Z","iopub.status.idle":"2024-05-06T20:13:02.322684Z","shell.execute_reply.started":"2024-05-06T20:13:02.315555Z","shell.execute_reply":"2024-05-06T20:13:02.321637Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_list = list(label_ids.values())\ndef compute_metrics(eval_preds): \n    pred_logits, labels = eval_preds \n    pred_logits = np.argmax(pred_logits, axis=2) \n    predictions = [ \n        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n        for prediction, label in zip(pred_logits, labels) \n    ] \n    \n    true_labels = [ \n      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n       for prediction, label in zip(pred_logits, labels) \n   ] \n    results = metric.compute(predictions=predictions, references=true_labels) \n    return { \n    \"precision\": results[\"overall_precision\"], \n    \"recall\": results[\"overall_recall\"], \n    \"f1\": results[\"overall_f1\"], \n    \"accuracy\": results[\"overall_accuracy\"], \n  } ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:13:05.420407Z","iopub.execute_input":"2024-05-06T20:13:05.420748Z","iopub.status.idle":"2024-05-06T20:13:05.428687Z","shell.execute_reply.started":"2024-05-06T20:13:05.420723Z","shell.execute_reply":"2024-05-06T20:13:05.427578Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def refine_input_features(model, dataset, tokenizer, tag_to_int):\n    model.eval()  # Set the model to evaluation mode to disable training-specific behaviors\n    refined_datasets = []\n\n#         print(data)\n    tokens = dataset['tokens']  # Accessing tokens directly\n    labels = dataset['labels']  # Accessing labels directly\n\n    # Tokenizing the tokens for model input\n    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n\n    # Select tokens and labels based on predictions not being 'O'\n    refined_tokens = [token for token, pred in zip(tokens, predictions) if tag_to_int[labels[pred]] != tag_to_int['O']]\n    refined_labels = [label for label, pred in zip(labels, predictions) if tag_to_int[label] != tag_to_int['O']]\n\n    refined_datasets.append({'tokens': refined_tokens, 'labels': refined_labels})\n\n    return refined_datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:21:01.159195Z","iopub.execute_input":"2024-05-06T13:21:01.159457Z","iopub.status.idle":"2024-05-06T13:21:01.171872Z","shell.execute_reply.started":"2024-05-06T13:21:01.159435Z","shell.execute_reply":"2024-05-06T13:21:01.171171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up training arguments\nargs = TrainingArguments(\n    \"Results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=15,\n    weight_decay=0.01,\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n    save_total_limit=5,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.1\n)\n\n# Initialize and train the model\nmodel = ArabNERModel.from_pretrained(model_path, num_labels=len(tag_to_int))\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nmodel = model.to(device)\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:13:15.683109Z","iopub.execute_input":"2024-05-06T20:13:15.683522Z","iopub.status.idle":"2024-05-06T20:26:39.114922Z","shell.execute_reply.started":"2024-05-06T20:13:15.683490Z","shell.execute_reply":"2024-05-06T20:26:39.113937Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0d95759d5e4686bfeec19538d6b5b2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4338' max='4338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4338/4338 13:19, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.649100</td>\n      <td>0.313538</td>\n      <td>0.818655</td>\n      <td>0.779891</td>\n      <td>0.798803</td>\n      <td>0.932160</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.278500</td>\n      <td>0.224475</td>\n      <td>0.850590</td>\n      <td>0.856658</td>\n      <td>0.853613</td>\n      <td>0.946358</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.188600</td>\n      <td>0.212040</td>\n      <td>0.853284</td>\n      <td>0.869226</td>\n      <td>0.861181</td>\n      <td>0.948773</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4338, training_loss=0.5607083990256212, metrics={'train_runtime': 800.0325, 'train_samples_per_second': 86.715, 'train_steps_per_second': 5.422, 'total_flos': 3288433713715920.0, 'train_loss': 0.5607083990256212, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model\nmetrics = trainer.evaluate(eval_dataset=tokenized_valid_ds)\nprint(f\"Evaluation metrics: {metrics}\")\n\n# Save the trained model\nmodel.save_pretrained(\"outer_layer_model\")\ntokenizer.save_pretrained(\"outer_layer_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:27:41.117960Z","iopub.execute_input":"2024-05-06T20:27:41.118349Z","iopub.status.idle":"2024-05-06T20:27:58.356318Z","shell.execute_reply.started":"2024-05-06T20:27:41.118313Z","shell.execute_reply":"2024-05-06T20:27:58.355339Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Evaluation metrics: {'eval_loss': 0.21204015612602234, 'eval_precision': 0.8532844281427142, 'eval_recall': 0.8692255434782609, 'eval_f1': 0.8611812216052498, 'eval_accuracy': 0.9487732207751168, 'eval_runtime': 15.6393, 'eval_samples_per_second': 211.263, 'eval_steps_per_second': 13.236, 'epoch': 3.0}\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('outer_layer_tokenizer/tokenizer_config.json',\n 'outer_layer_tokenizer/special_tokens_map.json',\n 'outer_layer_tokenizer/vocab.txt',\n 'outer_layer_tokenizer/added_tokens.json',\n 'outer_layer_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n\n# def train_sequential_layers(models, datasets, tokenizer, num_layers, tag_to_int, device):\n#     for layer in range(1, num_layers):  # Start from the second layer\n#         print(f\"Training layer {layer}\")\n#         # Assume refine_input_features returns a list of refined texts\n#         refined_dataset = refine_input_features(models[layer - 1], datasets[layer - 1], tokenizer, tag_to_int)\n\n#         # Prepare the new training dataset\n#         input_ids = tokenizer([data['tokens'] for data in refined_dataset], is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n#         labels = [data['labels'] for data in refined_dataset]  # Assuming labels need similar handling\n\n#         # Create Dataset objects for training\n#         train_dataset = Dataset.from_dict({'input_ids': input_ids, 'labels': labels})\n\n#         # Train a new model for this layer\n#         new_model = ArabNERModel.from_pretrained('aubmindlab/bert-base-arabertv2', num_labels=len(tag_to_int))\n#         new_model.to(device)  # Ensure the model is on the correct device\n        \n#         training_args = TrainingArguments(\n#             output_dir=f\"results_layer_{layer}\",\n#             evaluation_strategy=\"epoch\",\n#             learning_rate=2e-5,\n#             per_device_train_batch_size=8,\n#             num_train_epochs=3,\n#             save_strategy=\"no\",\n#             logging_dir=f\"logs_layer_{layer}\"  # Added logging directory for clarity\n#         )\n        \n#         trainer = Trainer(\n#             model=new_model,\n#             args=training_args,\n#             train_dataset=train_dataset,\n#             tokenizer=tokenizer,\n#             callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n#         )\n        \n#         # Train the model\n#         trainer.train()\n#         models.append(new_model)\n\n#     return models\n\n# # Assuming initial_model is pre-trained and ready\n# models = [model]\n# num_layers = 5  # Total number of layers including the initial pre-trained layer\n# trained_models = train_sequential_layers(models, train_datasets[1:], tokenizer, num_layers, tag_to_int, device)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:51:21.803084Z","iopub.status.idle":"2024-05-05T23:51:21.803401Z","shell.execute_reply.started":"2024-05-05T23:51:21.803249Z","shell.execute_reply":"2024-05-05T23:51:21.803262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n# # args = TrainingArguments(\n# #     \"Results\",\n# #     evaluation_strategy=\"epoch\",\n# #     save_strategy=\"epoch\",\n# #     learning_rate=2e-5,\n# #     per_device_train_batch_size=16,\n# #     per_device_eval_batch_size=16,\n# #     num_train_epochs=1,\n# #     weight_decay=0.01,\n# #     report_to=\"tensorboard\",\n# #     load_best_model_at_end=True,\n# #     save_total_limit=5,\n# #     lr_scheduler_type='linear',\n# #     warmup_ratio=0.1\n# # )\n\n# # # Initialize and train the model\n# # model = ArabNERModel.from_pretrained(model_path, num_labels=len(tag_to_int))\n\n# # trainer = Trainer(\n# #     model,\n# #     args,\n# #     train_dataset=tokenized_train_ds,\n# #     eval_dataset=tokenized_valid_ds,\n# #     data_collator=data_collator,\n# #     tokenizer=tokenizer,\n# #     compute_metrics=compute_metrics,\n# #     callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n# # )\n\n# def train_layer(model, tokenized_train_dataset, tokenized_valid_ds, tokenized_test_ds, tokenizer, layer_index, device):\n#     training_args = TrainingArguments(\n#         f\"Results_{layer_index}\",\n#         evaluation_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         learning_rate=2e-5,\n#         per_device_train_batch_size=16,\n#         per_device_eval_batch_size=16,\n#         num_train_epochs=1,\n#         weight_decay=0.01,\n#         report_to=\"tensorboard\",\n#         load_best_model_at_end=True,\n#         save_total_limit=5,\n#         lr_scheduler_type='linear',\n#         warmup_ratio=0.1\n#     )\n    \n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=tokenized_train_dataset,\n#         eval_dataset=tokenized_valid_ds,\n#         tokenizer=tokenizer,\n#         compute_metrics=compute_metrics,\n#         callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n#     )\n    \n#     # Train the model\n#     trainer.train()\n#     # Evaluate the model\n#     metrics = trainer.evaluate(eval_dataset=tokenized_test_ds)\n#     print(f\"Evaluation metrics: {metrics}\")\n    \n#     # Save model to disk\n#     model.save_pretrained(f\"saved_model_layer_{layer_index}\")\n    \n#     # Clear memory\n#     del model\n#     torch.cuda.empty_cache()\n\n# def train_sequential_layers(model_paths, train_datasets, validation_datasets, test_datasets, tokenizer, num_layers, device):\n#     models = []\n#     for layer in range(1, num_layers):\n#         print(f\"Training layer {layer}\")\n#         # Load the model for the current layer\n#         model = ArabNERModel.from_pretrained(model_paths[layer - 1])\n        \n#         tokenizer = BertTokenizerFast.from_pretrained(model_paths[layer - 1])\n#         data_collator = DataCollatorForTokenClassification(tokenizer) \n#         model.to(device)\n        \n#         if not train_datasets[layer-1] or not validation_datasets[layer-1] or not test_datasets[layer-1]:\n#             logging.warning(f\"Empty dataset for layer {layer}, skipping training.\")\n#             continue\n        \n#         train_ds = Dataset.from_dict(train_datasets[layer-1])\n#         valid_ds = Dataset.from_dict(validation_datasets[layer-1])\n#         test_ds = Dataset.from_dict(test_datasets[layer-1])\n#         tokenized_train_ds = train_ds.map(tokenize_and_align_labels, batched=True)\n#         tokenized_valid_ds = valid_ds.map(tokenize_and_align_labels, batched=True)\n#         tokenized_test_ds = test_ds.map(tokenize_and_align_labels, batched=True)\n        \n#         # Train the model\n#         train_layer(model, tokenized_train_ds, tokenized_valid_ds, tokenized_test_ds, tokenizer, layer, device)\n        \n#         # Append model path for next layer initialization\n#         models.append(f\"saved_model_layer_{layer}\")\n    \n#     return models\n\n# # Initial model path\n# # initial_model_path = \"initial_model_directory\"\n# # initial_model_path = \"/kaggle/working/Results/runs\"\n# num_layers = 5\n\n# model_paths = [model_path] * (num_layers - 1)  # Paths for later initialized layers\n# print(model_paths)\n# trained_models = train_sequential_layers(model_paths, train_datasets[1:], valid_datasets[1:], test_datasets[1:], tokenizer, num_layers, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:51:21.804632Z","iopub.status.idle":"2024-05-05T23:51:21.804937Z","shell.execute_reply.started":"2024-05-05T23:51:21.804788Z","shell.execute_reply":"2024-05-05T23:51:21.804800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()  # Clear cache before starting the training","metadata":{"execution":{"iopub.status.busy":"2024-05-05T21:31:19.677407Z","iopub.execute_input":"2024-05-05T21:31:19.678133Z","iopub.status.idle":"2024-05-05T21:31:19.682351Z","shell.execute_reply.started":"2024-05-05T21:31:19.678091Z","shell.execute_reply":"2024-05-05T21:31:19.681477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_last_created_folder(directory, prefix):\n    # Ensure the directory exists\n    if not os.path.exists(directory):\n        print(f\"The directory {directory} does not exist.\")\n        return None\n\n    # List all items in the directory\n    all_folders = [os.path.join(directory, f) for f in os.listdir(directory)]\n    # Filter list to include only directories that start with the specified prefix\n    folders = [folder for folder in all_folders if os.path.isdir(folder) and os.path.basename(folder).startswith(prefix)]\n    \n    # Check if the list is not empty\n    if not folders:\n        print(f\"No folders found in the directory that start with '{prefix}'.\")\n        return None\n\n    # Get the last created folder\n    last_created_folder = max(folders, key=os.path.getctime)\n\n    return last_created_folder\n\n# Path to the directory where folders are to be checked\ndirectory_path = '/kaggle/input/model-outputs-version-5/Results'\nfolder_prefix = 'checkpoint-18798'  # The prefix to look for in folder names\ncheckpoint_path = find_last_created_folder(directory_path, folder_prefix)\n# checkpoint_path = '/kaggle/input/model-outputs-version-5/Results/checkpoint-18798'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:50:54.407892Z","iopub.execute_input":"2024-05-06T19:50:54.408773Z","iopub.status.idle":"2024-05-06T19:50:54.421526Z","shell.execute_reply.started":"2024-05-06T19:50:54.408741Z","shell.execute_reply":"2024-05-06T19:50:54.420608Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# test_model = ArabNERModelWithHybridLoss.from_pretrained(checkpoint_path, num_labels=len(label_ids.values()))\n# test_model = ArabNERModelWithWeightedLoss.from_pretrained(checkpoint_path, num_labels=len(tag_to_int))\n# tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:50:57.383685Z","iopub.execute_input":"2024-05-06T19:50:57.384063Z","iopub.status.idle":"2024-05-06T19:50:57.693788Z","shell.execute_reply.started":"2024-05-06T19:50:57.384034Z","shell.execute_reply":"2024-05-06T19:50:57.692714Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"inverse_label_ids = {v: k for k, v in label_ids.items()}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:56:14.187148Z","iopub.execute_input":"2024-05-06T19:56:14.188050Z","iopub.status.idle":"2024-05-06T19:56:14.192713Z","shell.execute_reply.started":"2024-05-06T19:56:14.188018Z","shell.execute_reply":"2024-05-06T19:56:14.191613Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndef predict_in_batches(model, tokenized_inputs, batch_size=32):\n    # Check for GPU availability\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)  # Move model to the appropriate device\n    model.eval()\n    \n    all_predictions = []\n    input_ids = tokenized_inputs['input_ids']\n    attention_mask = tokenized_inputs['attention_mask']\n\n    # Wrap the range with tqdm for a progress bar\n    for i in tqdm(range(0, input_ids.size(0), batch_size)):\n        batch_input_ids = input_ids[i:i + batch_size].to(device)\n        batch_attention_mask = attention_mask[i:i + batch_size].to(device)\n        \n        with torch.no_grad():\n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n            logits = outputs.logits\n            predictions = logits.argmax(-1)\n        \n        all_predictions.append(predictions.cpu())  # Move predictions back to CPU if needed\n\n    return torch.cat(all_predictions, dim=0)\n\ndef map_predictions_to_original_tokens(predictions, tokenized_inputs, label_ids):\n    word_level_predictions = []\n    offset_mappings = tokenized_inputs['offset_mapping']\n    word_ids = [tokenized_inputs.word_ids(batch_index=i) for i in range(predictions.size(0))]\n\n    for idx, (preds, length) in enumerate(zip(predictions, tokenized_inputs['attention_mask'].sum(1))):\n        current_word_ids = word_ids[idx]\n        word_predictions = []\n        previous_word_idx = None\n        for word_idx, pred in zip(current_word_ids, preds[:length]):\n            if word_idx is not None and word_idx != previous_word_idx:\n                word_predictions.append(label_ids[pred.item()])\n            previous_word_idx = word_idx\n        word_level_predictions.append(word_predictions)\n\n    return word_level_predictions\n\ntokenized_inputs = tokenizer(test_ds['tokens'], is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n# predictions = predict_in_batches(test_model, tokenized_inputs, batch_size=16)\npredictions = predict_in_batches(model, tokenized_inputs, batch_size=16)\nword_level_predictions = map_predictions_to_original_tokens(predictions, tokenized_inputs, label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:27:58.357858Z","iopub.execute_input":"2024-05-06T20:27:58.358307Z","iopub.status.idle":"2024-05-06T20:29:51.594022Z","shell.execute_reply.started":"2024-05-06T20:27:58.358264Z","shell.execute_reply":"2024-05-06T20:29:51.592918Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [01:44<00:00,  3.95it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# print(label_ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:31:19.715982Z","iopub.execute_input":"2024-05-06T20:31:19.716703Z","iopub.status.idle":"2024-05-06T20:31:19.720778Z","shell.execute_reply.started":"2024-05-06T20:31:19.716665Z","shell.execute_reply":"2024-05-06T20:31:19.719841Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:02:52.890981Z","iopub.execute_input":"2024-05-06T20:02:52.891852Z","iopub.status.idle":"2024-05-06T20:02:52.895765Z","shell.execute_reply.started":"2024-05-06T20:02:52.891816Z","shell.execute_reply":"2024-05-06T20:02:52.894721Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def format_predictions_to_conll(tokens_list, word_level_predictions):\n    \"\"\"Format the predictions to the CoNLL output format.\"\"\"\n    output = []\n    for tokens, predictions in zip(tokens_list, word_level_predictions):\n        for token, tag in zip(tokens, predictions):\n            output.append(f\"{token} {tag}\")\n        output.append(\"\")  # Add a blank line after each sentence for segment separation\n    return \"\\n\".join(output)\n\ndef write_to_file(content, filename):\n    \"\"\"Write the given content to a text file.\"\"\"\n    with open(filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(content)\n    print(f\"Data written to {filename}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:29:51.595791Z","iopub.execute_input":"2024-05-06T20:29:51.596079Z","iopub.status.idle":"2024-05-06T20:29:51.604147Z","shell.execute_reply.started":"2024-05-06T20:29:51.596052Z","shell.execute_reply":"2024-05-06T20:29:51.603196Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"conll_output = format_predictions_to_conll(test_ds['tokens'], word_level_predictions)\nwrite_to_file(conll_output, \"/kaggle/working/ArabNER_subtask2_valid_pred_2_new.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T20:29:51.605203Z","iopub.execute_input":"2024-05-06T20:29:51.605461Z","iopub.status.idle":"2024-05-06T20:29:51.852615Z","shell.execute_reply.started":"2024-05-06T20:29:51.605438Z","shell.execute_reply":"2024-05-06T20:29:51.851658Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Data written to /kaggle/working/ArabNER_subtask2_valid_pred_2_new.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}