{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8303505,"sourceType":"datasetVersion","datasetId":4932833},{"sourceId":8329960,"sourceType":"datasetVersion","datasetId":4946162}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip install seqeval transformers datasets tokenizers seqeval evaluate\n! pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T22:31:18.110588Z","iopub.execute_input":"2024-05-05T22:31:18.111258Z","iopub.status.idle":"2024-05-05T22:31:34.481938Z","shell.execute_reply.started":"2024-05-05T22:31:18.111227Z","shell.execute_reply":"2024-05-05T22:31:34.480940Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=c5f22c4f4cbcfae7ea07e871ed6db418418d020a3b7042586aed501bd9505010\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport datasets\nimport json\nimport numpy as np \nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForTokenClassification, BertConfig, DataCollatorForTokenClassification, BertTokenizerFast, TrainingArguments, Trainer, EarlyStoppingCallback\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:31:34.483943Z","iopub.execute_input":"2024-05-05T22:31:34.484255Z","iopub.status.idle":"2024-05-05T22:31:52.028011Z","shell.execute_reply.started":"2024-05-05T22:31:34.484228Z","shell.execute_reply":"2024-05-05T22:31:52.027041Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-05 22:31:43.161648: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-05 22:31:43.161743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-05 22:31:43.287533: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_arabic(text):\n    alif_maksura_to_yeh = re.sub(r'[ÙŠÙ‰]', 'ÙŠ', text)\n    teh_marbuta_to_heh = re.sub(r'Ø©', 'Ù‡', alif_maksura_to_yeh)\n    alifs_normalized = re.sub(r'[Ø£Ø¥Ø¢]', 'Ø§', teh_marbuta_to_heh)\n    kafs_normalized = re.sub(r'Ú©', 'Ùƒ', alifs_normalized)\n    text_cleaned = re.sub(r'[\\u064B-\\u065F]', '', kafs_normalized)\n\n    return text_cleaned","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_json_file_nested(file_path, tag_to_int):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    token_data_layers = []\n    label_data_layers = []\n    max_depth = 0\n\n    for sentence in data:\n        for token_info in sentence['tokens']:\n            max_depth = max(max_depth, len(token_info['tags']))\n\n    for _ in range(max_depth):\n        token_data_layers.append([])\n        label_data_layers.append([])\n\n    for sentence in data:\n        for depth in range(max_depth):\n            token_list = []\n            label_list = []\n            for token_info in sentence['tokens']:\n                token = token_info['token']\n                if depth < len(token_info['tags']):\n                    tag_info = token_info['tags'][depth]\n                    value = tag_info['value']\n                else:\n                    value = \"O\"\n                token_list.append(token)\n                label_list.append(tag_to_int[value])\n            token_data_layers[depth].append(token_list)\n            label_data_layers[depth].append(label_list)\n\n    datasets = []\n    for i in range(max_depth):\n        datasets.append({'tokens': token_data_layers[i], 'labels': label_data_layers[i]})\n\n    return datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:31:52.029216Z","iopub.execute_input":"2024-05-05T22:31:52.029768Z","iopub.status.idle":"2024-05-05T22:31:52.039293Z","shell.execute_reply.started":"2024-05-05T22:31:52.029741Z","shell.execute_reply":"2024-05-05T22:31:52.038445Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/ner-nested/split70.json'\nvalid_path = '/kaggle/input/ner-nested/split10.json'\ntest_path = '/kaggle/input/ner-nested/split10.json'","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:31:52.042089Z","iopub.execute_input":"2024-05-05T22:31:52.042556Z","iopub.status.idle":"2024-05-05T22:31:52.067567Z","shell.execute_reply.started":"2024-05-05T22:31:52.042520Z","shell.execute_reply":"2024-05-05T22:31:52.066443Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef extract_unique_tags(file_paths):\n    unique_tags = set()\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n            for sentence in data:\n                for token_info in sentence['tokens']:\n                    for tag_info in token_info['tags']:\n                        unique_tags.add(tag_info['value'])\n                        if tag_info.get('tags'):\n                            for nested_tag in tag_info['tags']:\n                                unique_tags.add(nested_tag['value'])\n    return unique_tags\n\n# Define file paths for your training, validation, and test datasets\nfile_paths = [train_path, valid_path, test_path]\nunique_tags = extract_unique_tags(file_paths)\ntag_to_int = {tag: idx for idx, tag in enumerate(unique_tags)}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:31:52.068636Z","iopub.execute_input":"2024-05-05T22:31:52.068944Z","iopub.status.idle":"2024-05-05T22:31:57.549939Z","shell.execute_reply.started":"2024-05-05T22:31:52.068918Z","shell.execute_reply":"2024-05-05T22:31:57.548849Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load datasets for the first layer only\ntrain_datasets = process_json_file_nested(train_path, tag_to_int)\nvalid_datasets = process_json_file_nested(valid_path, tag_to_int)\ntest_datasets = process_json_file_nested(test_path, tag_to_int)\nlabel_ids = {idx: label for label, idx in tag_to_int.items()}\n\ntrain_ds = Dataset.from_dict(train_datasets[0])\nvalid_ds = Dataset.from_dict(valid_datasets[0])\ntest_ds = Dataset.from_dict(test_datasets[0])\n\ntrain_ds_layers = [Dataset.from_dict(layer) for layer in train_datasets]\nvalid_ds_layers = [Dataset.from_dict(layer) for layer in valid_datasets]\ntest_ds_layers = [Dataset.from_dict(layer) for layer in test_datasets]\n\n# Select the first layer (index 0)\n# train_dataset = train_datasets[0]\n# valid_dataset = valid_datasets[0]\n# test_dataset = test_datasets[0]\n# tag_to_int = {label: idx for idx, label in enumerate(set([lbl for sublist in train_dataset['labels'] for lbl in sublist]))}\n# label_ids = {idx: label for label, idx in tag_to_int.items()}\n\n# train_ds = Dataset.from_dict(train_dataset)\n# valid_ds = Dataset.from_dict(valid_dataset)\n# test_ds = Dataset.from_dict(test_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:31:57.551076Z","iopub.execute_input":"2024-05-05T22:31:57.551346Z","iopub.status.idle":"2024-05-05T22:32:04.939422Z","shell.execute_reply.started":"2024-05-05T22:31:57.551324Z","shell.execute_reply":"2024-05-05T22:32:04.938436Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples, label_all_tokens=True):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"labels\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:04.940982Z","iopub.execute_input":"2024-05-05T22:32:04.941287Z","iopub.status.idle":"2024-05-05T22:32:04.948275Z","shell.execute_reply.started":"2024-05-05T22:32:04.941263Z","shell.execute_reply":"2024-05-05T22:32:04.947249Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_path = \"aubmindlab/bert-base-arabertv2\"\ntokenizer = BertTokenizerFast.from_pretrained(model_path)\ndata_collator = DataCollatorForTokenClassification(tokenizer) \nmetric = datasets.load_metric(\"seqeval\") ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:04.949873Z","iopub.execute_input":"2024-05-05T22:32:04.950187Z","iopub.status.idle":"2024-05-05T22:32:06.258844Z","shell.execute_reply.started":"2024-05-05T22:32:04.950163Z","shell.execute_reply":"2024-05-05T22:32:06.257942Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bc0acab76bf487da657d62b1022357a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/720k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67918ff9cc054f2898efca47a9ed89ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ff567296b245a8847de26194691a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f0853c4152d4e808dcee3530da1bd0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398609250fd24fdba7b19c5575ae6677"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_34/2027728299.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = datasets.load_metric(\"seqeval\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b24fc6a0c814f3a8c65be953242e410"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_prepare(dataset):\n    return dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:06.260038Z","iopub.execute_input":"2024-05-05T22:32:06.260353Z","iopub.status.idle":"2024-05-05T22:32:06.264802Z","shell.execute_reply.started":"2024-05-05T22:32:06.260328Z","shell.execute_reply":"2024-05-05T22:32:06.263904Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds.map(tokenize_and_align_labels, batched=True)\ntokenized_valid_ds = valid_ds.map(tokenize_and_align_labels, batched=True)\ntokenized_test_ds = test_ds.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:06.268366Z","iopub.execute_input":"2024-05-05T22:32:06.269287Z","iopub.status.idle":"2024-05-05T22:32:10.804311Z","shell.execute_reply.started":"2024-05-05T22:32:06.269252Z","shell.execute_reply":"2024-05-05T22:32:10.803433Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23125 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ea17085ec9481ab731f3a90d34a038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3304 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c371f8fe36b54ae3b17a0aa7e8b496de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3304 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b691e94994b54b31ac4bdf9b0c08ff9c"}},"metadata":{}}]},{"cell_type":"code","source":"class ArabNERModel(AutoModelForTokenClassification):\n    def __init__(self, config):\n        super().__init__(config)\n        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n\n    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n        outputs = super().forward(input_ids, attention_mask=attention_mask, **kwargs)\n        if labels is not None:\n            loss = self.loss(outputs.logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs[1:]\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:10.805567Z","iopub.execute_input":"2024-05-05T22:32:10.805888Z","iopub.status.idle":"2024-05-05T22:32:10.812572Z","shell.execute_reply.started":"2024-05-05T22:32:10.805862Z","shell.execute_reply":"2024-05-05T22:32:10.811541Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"label_list = list(label_ids.values())\ndef compute_metrics(eval_preds): \n    pred_logits, labels = eval_preds \n    pred_logits = np.argmax(pred_logits, axis=2) \n    predictions = [ \n        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n        for prediction, label in zip(pred_logits, labels) \n    ] \n    \n    true_labels = [ \n      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n       for prediction, label in zip(pred_logits, labels) \n   ] \n    results = metric.compute(predictions=predictions, references=true_labels) \n    return { \n    \"precision\": results[\"overall_precision\"], \n    \"recall\": results[\"overall_recall\"], \n    \"f1\": results[\"overall_f1\"], \n    \"accuracy\": results[\"overall_accuracy\"], \n  } ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:10.813823Z","iopub.execute_input":"2024-05-05T22:32:10.814068Z","iopub.status.idle":"2024-05-05T22:32:10.824426Z","shell.execute_reply.started":"2024-05-05T22:32:10.814046Z","shell.execute_reply":"2024-05-05T22:32:10.823610Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def refine_input_features(model, dataset, tokenizer, tag_to_int):\n    model.eval()  # Set the model to evaluation mode to disable training-specific behaviors\n    refined_datasets = []\n\n#         print(data)\n    tokens = dataset['tokens']  # Accessing tokens directly\n    labels = dataset['labels']  # Accessing labels directly\n\n    # Tokenizing the tokens for model input\n    inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predictions = outputs.logits.argmax(-1).squeeze().tolist()\n\n    # Select tokens and labels based on predictions not being 'O'\n    refined_tokens = [token for token, pred in zip(tokens, predictions) if tag_to_int[labels[pred]] != tag_to_int['O']]\n    refined_labels = [label for label, pred in zip(labels, predictions) if tag_to_int[label] != tag_to_int['O']]\n\n    refined_datasets.append({'tokens': refined_tokens, 'labels': refined_labels})\n\n    return refined_datasets\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T22:32:10.825429Z","iopub.execute_input":"2024-05-05T22:32:10.825737Z","iopub.status.idle":"2024-05-05T22:32:10.839836Z","shell.execute_reply.started":"2024-05-05T22:32:10.825714Z","shell.execute_reply":"2024-05-05T22:32:10.839072Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Set up training arguments\nargs = TrainingArguments(\n    \"Results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=30,\n    weight_decay=0.01,\n    report_to=\"tensorboard\",\n    load_best_model_at_end=True,\n    save_total_limit=5,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.1\n)\n\n# Initialize and train the model\nmodel = ArabNERModel.from_pretrained(model_path, num_labels=len(tag_to_int))\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\nmodel = model.to(device)\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:29:34.114366Z","iopub.execute_input":"2024-05-05T23:29:34.114675Z","iopub.status.idle":"2024-05-05T23:51:21.799705Z","shell.execute_reply.started":"2024-05-05T23:29:34.114650Z","shell.execute_reply":"2024-05-05T23:51:21.798139Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7231' max='43380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 7231/43380 21:39 < 1:48:19, 5.56 it/s, Epoch 5/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.326400</td>\n      <td>0.488925</td>\n      <td>0.728722</td>\n      <td>0.671565</td>\n      <td>0.698977</td>\n      <td>0.902456</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.347600</td>\n      <td>0.218906</td>\n      <td>0.801759</td>\n      <td>0.843131</td>\n      <td>0.821924</td>\n      <td>0.940027</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192200</td>\n      <td>0.176181</td>\n      <td>0.838428</td>\n      <td>0.878715</td>\n      <td>0.858099</td>\n      <td>0.948389</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.122700</td>\n      <td>0.170293</td>\n      <td>0.849036</td>\n      <td>0.890604</td>\n      <td>0.869323</td>\n      <td>0.951121</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='104' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [104/207 00:04 < 00:04, 24.94 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:158\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: BatchEncoding.to() got an unexpected keyword argument 'non_blocking'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2213\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2217\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2577\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2577\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2580\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3365\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3362\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3364\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3365\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3369\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3375\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3544\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3542\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3543\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 3544\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   3545\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   3546\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   3547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:461\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# This call is inside the try-block since is_npu_available is not supported by torch.compile.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_npu_available():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:800\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Evaluate the model\nmetrics = trainer.evaluate(eval_dataset=tokenized_test_ds)\nprint(f\"Evaluation metrics: {metrics}\")\n\n# Save the trained model\nmodel.save_pretrained(\"outer_layer_model\")\ntokenizer.save_pretrained(\"outer_layer_tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:51:21.800845Z","iopub.status.idle":"2024-05-05T23:51:21.801332Z","shell.execute_reply.started":"2024-05-05T23:51:21.801096Z","shell.execute_reply":"2024-05-05T23:51:21.801118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n\n# def train_sequential_layers(models, datasets, tokenizer, num_layers, tag_to_int, device):\n#     for layer in range(1, num_layers):  # Start from the second layer\n#         print(f\"Training layer {layer}\")\n#         # Assume refine_input_features returns a list of refined texts\n#         refined_dataset = refine_input_features(models[layer - 1], datasets[layer - 1], tokenizer, tag_to_int)\n\n#         # Prepare the new training dataset\n#         input_ids = tokenizer([data['tokens'] for data in refined_dataset], is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n#         labels = [data['labels'] for data in refined_dataset]  # Assuming labels need similar handling\n\n#         # Create Dataset objects for training\n#         train_dataset = Dataset.from_dict({'input_ids': input_ids, 'labels': labels})\n\n#         # Train a new model for this layer\n#         new_model = ArabNERModel.from_pretrained('aubmindlab/bert-base-arabertv2', num_labels=len(tag_to_int))\n#         new_model.to(device)  # Ensure the model is on the correct device\n        \n#         training_args = TrainingArguments(\n#             output_dir=f\"results_layer_{layer}\",\n#             evaluation_strategy=\"epoch\",\n#             learning_rate=2e-5,\n#             per_device_train_batch_size=8,\n#             num_train_epochs=3,\n#             save_strategy=\"no\",\n#             logging_dir=f\"logs_layer_{layer}\"  # Added logging directory for clarity\n#         )\n        \n#         trainer = Trainer(\n#             model=new_model,\n#             args=training_args,\n#             train_dataset=train_dataset,\n#             tokenizer=tokenizer,\n#             callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n#         )\n        \n#         # Train the model\n#         trainer.train()\n#         models.append(new_model)\n\n#     return models\n\n# # Assuming initial_model is pre-trained and ready\n# models = [model]\n# num_layers = 5  # Total number of layers including the initial pre-trained layer\n# trained_models = train_sequential_layers(models, train_datasets[1:], tokenizer, num_layers, tag_to_int, device)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:51:21.803084Z","iopub.status.idle":"2024-05-05T23:51:21.803401Z","shell.execute_reply.started":"2024-05-05T23:51:21.803249Z","shell.execute_reply":"2024-05-05T23:51:21.803262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments, Trainer\n# # args = TrainingArguments(\n# #     \"Results\",\n# #     evaluation_strategy=\"epoch\",\n# #     save_strategy=\"epoch\",\n# #     learning_rate=2e-5,\n# #     per_device_train_batch_size=16,\n# #     per_device_eval_batch_size=16,\n# #     num_train_epochs=1,\n# #     weight_decay=0.01,\n# #     report_to=\"tensorboard\",\n# #     load_best_model_at_end=True,\n# #     save_total_limit=5,\n# #     lr_scheduler_type='linear',\n# #     warmup_ratio=0.1\n# # )\n\n# # # Initialize and train the model\n# # model = ArabNERModel.from_pretrained(model_path, num_labels=len(tag_to_int))\n\n# # trainer = Trainer(\n# #     model,\n# #     args,\n# #     train_dataset=tokenized_train_ds,\n# #     eval_dataset=tokenized_valid_ds,\n# #     data_collator=data_collator,\n# #     tokenizer=tokenizer,\n# #     compute_metrics=compute_metrics,\n# #     callbacks=[EarlyStoppingCallback(early_stopping_patience=8)]\n# # )\n\n# def train_layer(model, tokenized_train_dataset, tokenized_valid_ds, tokenized_test_ds, tokenizer, layer_index, device):\n#     training_args = TrainingArguments(\n#         f\"Results_{layer_index}\",\n#         evaluation_strategy=\"epoch\",\n#         save_strategy=\"epoch\",\n#         learning_rate=2e-5,\n#         per_device_train_batch_size=16,\n#         per_device_eval_batch_size=16,\n#         num_train_epochs=1,\n#         weight_decay=0.01,\n#         report_to=\"tensorboard\",\n#         load_best_model_at_end=True,\n#         save_total_limit=5,\n#         lr_scheduler_type='linear',\n#         warmup_ratio=0.1\n#     )\n    \n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=tokenized_train_dataset,\n#         eval_dataset=tokenized_valid_ds,\n#         tokenizer=tokenizer,\n#         compute_metrics=compute_metrics,\n#         callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n#     )\n    \n#     # Train the model\n#     trainer.train()\n#     # Evaluate the model\n#     metrics = trainer.evaluate(eval_dataset=tokenized_test_ds)\n#     print(f\"Evaluation metrics: {metrics}\")\n    \n#     # Save model to disk\n#     model.save_pretrained(f\"saved_model_layer_{layer_index}\")\n    \n#     # Clear memory\n#     del model\n#     torch.cuda.empty_cache()\n\n# def train_sequential_layers(model_paths, train_datasets, validation_datasets, test_datasets, tokenizer, num_layers, device):\n#     models = []\n#     for layer in range(1, num_layers):\n#         print(f\"Training layer {layer}\")\n#         # Load the model for the current layer\n#         model = ArabNERModel.from_pretrained(model_paths[layer - 1])\n        \n#         tokenizer = BertTokenizerFast.from_pretrained(model_paths[layer - 1])\n#         data_collator = DataCollatorForTokenClassification(tokenizer) \n#         model.to(device)\n        \n#         if not train_datasets[layer-1] or not validation_datasets[layer-1] or not test_datasets[layer-1]:\n#             logging.warning(f\"Empty dataset for layer {layer}, skipping training.\")\n#             continue\n        \n#         train_ds = Dataset.from_dict(train_datasets[layer-1])\n#         valid_ds = Dataset.from_dict(validation_datasets[layer-1])\n#         test_ds = Dataset.from_dict(test_datasets[layer-1])\n#         tokenized_train_ds = train_ds.map(tokenize_and_align_labels, batched=True)\n#         tokenized_valid_ds = valid_ds.map(tokenize_and_align_labels, batched=True)\n#         tokenized_test_ds = test_ds.map(tokenize_and_align_labels, batched=True)\n        \n#         # Train the model\n#         train_layer(model, tokenized_train_ds, tokenized_valid_ds, tokenized_test_ds, tokenizer, layer, device)\n        \n#         # Append model path for next layer initialization\n#         models.append(f\"saved_model_layer_{layer}\")\n    \n#     return models\n\n# # Initial model path\n# # initial_model_path = \"initial_model_directory\"\n# # initial_model_path = \"/kaggle/working/Results/runs\"\n# num_layers = 5\n\n# model_paths = [model_path] * (num_layers - 1)  # Paths for later initialized layers\n# print(model_paths)\n# trained_models = train_sequential_layers(model_paths, train_datasets[1:], valid_datasets[1:], test_datasets[1:], tokenizer, num_layers, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T23:51:21.804632Z","iopub.status.idle":"2024-05-05T23:51:21.804937Z","shell.execute_reply.started":"2024-05-05T23:51:21.804788Z","shell.execute_reply":"2024-05-05T23:51:21.804800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()  # Clear cache before starting the training","metadata":{"execution":{"iopub.status.busy":"2024-05-05T21:31:19.677407Z","iopub.execute_input":"2024-05-05T21:31:19.678133Z","iopub.status.idle":"2024-05-05T21:31:19.682351Z","shell.execute_reply.started":"2024-05-05T21:31:19.678091Z","shell.execute_reply":"2024-05-05T21:31:19.681477Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}